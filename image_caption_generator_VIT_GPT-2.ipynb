{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.json',\n",
       " './saved_model/merges.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model, feature extractor, and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define directory to save the model\n",
    "save_directory = \"./saved_model\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the feature extractor\n",
    "feature_extractor.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(save_directory)\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(save_directory)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generation parameters\n",
    "\n",
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image_paths(folder):\n",
    "#     image_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")\n",
    "#     return [os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(image_extensions)]\n",
    "\n",
    "# def save_captions(output_file, captions):\n",
    "#     with open(output_file, \"w\") as f:\n",
    "#         for image_path, caption in captions:\n",
    "#             f.write(f\"{image_path}\\t{caption}\\n\")\n",
    "\n",
    "# class ImageCaptioningModel:\n",
    "#     def __init__(self):\n",
    "#         self.model = model\n",
    "#         self.feature_extractor = feature_extractor\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = device\n",
    "#         self.gen_kwargs = gen_kwargs\n",
    "\n",
    "#     def predict_step(self, image_paths):\n",
    "#         images = []\n",
    "#         for image_path in image_paths:\n",
    "#             try:\n",
    "#                 i_image = Image.open(image_path)\n",
    "#                 if i_image.mode != \"RGB\":\n",
    "#                     i_image = i_image.convert(mode=\"RGB\")\n",
    "#                 images.append(i_image)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading image {image_path}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         if not images:\n",
    "#             return []\n",
    "\n",
    "#         pixel_values = self.feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "#         pixel_values = pixel_values.to(self.device)\n",
    "\n",
    "#         output_ids = self.model.generate(pixel_values, **self.gen_kwargs)\n",
    "\n",
    "#         preds = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "#         preds = [pred.strip() for pred in preds]\n",
    "#         return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "  images = []\n",
    "  for image_path in image_paths:\n",
    "    i_image = Image.open(image_path)\n",
    "    if i_image.mode != \"RGB\":\n",
    "      i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "    images.append(i_image)\n",
    "\n",
    "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "  pixel_values = pixel_values.to(device)\n",
    "\n",
    "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "  preds = [pred.strip() for pred in preds]\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a man with a beard wearing a green jacket']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_step(['headshot_Ishak.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "image_folder = \"picture\"  # Update this path\n",
    "output_file = \"captions.txt\"\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize the model\n",
    "caption_model = ImageCaptioningModel()\n",
    "\n",
    "# Load image paths\n",
    "image_paths = load_image_paths(image_folder)\n",
    "\n",
    "# Generate captions\n",
    "all_captions = []\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i + batch_size]\n",
    "    captions = caption_model.predict_step(batch_paths)\n",
    "    all_captions.extend(zip(batch_paths, captions))\n",
    "\n",
    "# Save captions to a file\n",
    "save_captions(output_file, all_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Path</th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>picture/hello3.jpg</td>\n",
       "      <td>a black and white dog and a black and white dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>picture/headshot_Ishak.jpg</td>\n",
       "      <td>a man with a beard wearing a green jacket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Image Path                                          Caption\n",
       "0          picture/hello3.jpg  a black and white dog and a black and white dog\n",
       "1  picture/headshot_Ishak.jpg        a man with a beard wearing a green jacket"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and display some of the results\n",
    "captions_df = pd.DataFrame(all_captions, columns=[\"Image Path\", \"Caption\"])\n",
    "captions_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
